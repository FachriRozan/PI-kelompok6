{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Create a Sastrawi stemmer\n",
    "stem_factory = StemmerFactory()\n",
    "stemmer = stem_factory.create_stemmer()\n",
    "\n",
    "# Function to read and preprocess documents\n",
    "def preprocess_document(file_path, stemmer, stopwords_ind):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        # Convert to lowercase\n",
    "        content = content.lower()\n",
    "        # Tokenize the content\n",
    "        words = word_tokenize(content)\n",
    "        # Remove punctuation and keep alphanumeric tokens\n",
    "        words = [word for word in words if word.isalnum()]\n",
    "        # Stem the words using Sastrawi\n",
    "        stemmed_words = [stemmer.stem(word) for word in words]\n",
    "        # Extract stopwords from the document\n",
    "        stopwords_from_doc = [word for word in stemmed_words if word in stopwords_ind]\n",
    "\n",
    "        # Extract URL if available\n",
    "        urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', content)\n",
    "        \n",
    "        return ' '.join(stemmed_words), stopwords_from_doc, urls\n",
    "\n",
    "# Get a list of Indonesian stopwords\n",
    "stopwords_ind = set(stopwords.words('indonesian'))\n",
    "\n",
    "# Define the corpus folder\n",
    "corpus_folder = 'Koprus_PI'  # Replace with your corpus folder path\n",
    "\n",
    "# Create a directory to store tokenized, stemmed words and URLs\n",
    "output_dir = 'processed_texts'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Lists to store the extracted URLs and their corresponding documents\n",
    "document_urls = []\n",
    "\n",
    "# Tokenize, stem, extract URLs, and save the processed words and URLs to files\n",
    "for filename in os.listdir(corpus_folder):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(corpus_folder, filename)\n",
    "        content, doc_stopwords, urls = preprocess_document(file_path, stemmer, stopwords_ind)\n",
    "        \n",
    "        # Save the processed words to a file\n",
    "        output_file_path = os.path.join(output_dir, f'{os.path.splitext(filename)[0]}_processed.txt')\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            output_file.write(content)\n",
    "        \n",
    "        # Store the extracted URLs for each document\n",
    "        document_urls.append((filename, urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load processed words from files\n",
    "def load_processed_words(directory):\n",
    "    processed_words = []\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith('_processed.txt'):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                processed_words.append(file.read())\n",
    "    return processed_words\n",
    "\n",
    "# Load processed words from the 'processed_texts' directory\n",
    "processed_words = load_processed_words(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dictionary to store the index (word to document mapping)\n",
    "word_to_doc_index = {}\n",
    "\n",
    "# Index the documents and build the word-to-document index\n",
    "for idx, content in enumerate(processed_words):\n",
    "    # Split the content into words\n",
    "    words = content.split()\n",
    "    # For each word, update the index\n",
    "    for word in words:\n",
    "        if word not in word_to_doc_index:\n",
    "            word_to_doc_index[word] = []\n",
    "        # Append the document index to the word's index\n",
    "        word_to_doc_index[word].append(idx)\n",
    "\n",
    "import json\n",
    "\n",
    "# Path to save the index file\n",
    "index_file_path = 'word_to_doc_index.json'\n",
    "\n",
    "# Save the word-to-document index to a JSON file\n",
    "with open(index_file_path, 'w', encoding='utf-8') as index_file:\n",
    "    json.dump(word_to_doc_index, index_file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Combine stopwords from documents with Indonesian stopwords\n",
    "combined_stopwords = list(stopwords_ind) + doc_stopwords # Using doc_stopwords from Cell 1\n",
    "\n",
    "# Calculate TF-IDF for the processed words\n",
    "vectorizer = TfidfVectorizer(stop_words=combined_stopwords)  # Use the combined stopwords\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Title: Surah_Maryam\n",
      "Similarity Score: 0.1744060650355325\n",
      "==================================================\n",
      "2\n",
      "Title: Zakariyya\n",
      "Similarity Score: 0.03364531524311676\n",
      "==================================================\n",
      "3\n",
      "Title: Surah_al-Anbiya_27\n",
      "Similarity Score: 0.031469276378022173\n",
      "==================================================\n",
      "4\n",
      "Title: Isra_Mikraj\n",
      "Similarity Score: 0.024479111348216575\n",
      "==================================================\n",
      "5\n",
      "Title: Yahya\n",
      "Similarity Score: 0.01297160756268748\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import webbrowser\n",
    "\n",
    "# Create a query\n",
    "query = input('Enter search keywords: ')\n",
    "query = query.lower()\n",
    "\n",
    "# Transform the query into TF-IDF representation\n",
    "query_vector = vectorizer.transform([query])\n",
    "\n",
    "# Calculate similarity scores (cosine similarity) between the query and documents\n",
    "cosine_similarities = tfidf_matrix.dot(query_vector.T).toarray().flatten()\n",
    "\n",
    "# Sort the documents based on similarity scores\n",
    "sorted_document_indices = cosine_similarities.argsort()[::-1][:5]\n",
    "\n",
    "# Display the results (title, URL, and stopwords from the document)\n",
    "j = 0\n",
    "for i in sorted_document_indices:\n",
    "    j += 1\n",
    "    print(j)\n",
    "    print(\"Title:\", os.path.splitext(os.path.basename(os.listdir(corpus_folder)[i]))[0])\n",
    "    print(\"Similarity Score:\", cosine_similarities[i])\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        # Ask for input as a number from the user\n",
    "        num = int(input(\"Select the desired website number (1-5): \"))\n",
    "\n",
    "        # Check if the input is within the range 1-5\n",
    "        if 1 <= num <= 5:\n",
    "            # Get the document index corresponding to the user's choice\n",
    "            selected_index = sorted_document_indices[num - 1]\n",
    "\n",
    "            # Open the URLs associated with the selected document\n",
    "            for url in document_urls[selected_index][1]:\n",
    "                webbrowser.open(url)  # Access the URLs from the tuple and open each one\n",
    "            break  # Exit the loop after opening the URLs\n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter a number between 1 and 5.\")\n",
    "    except ValueError:\n",
    "        print(\"Please enter a valid number.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
