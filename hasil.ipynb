{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dokumen teratas untuk query 'muhammad' berdasarkan Jaccard similarity adalah:\n",
      "Dokumen 140:\n",
      "Skor Jaccard similarity: 1.0\n",
      "Nama Dokumen: Puasa.txt\n",
      "Dokumen: *muhammad*\n",
      "Dokumen 304:\n",
      "Skor Jaccard similarity: 1.0\n",
      "Nama Dokumen: Yahya.txt\n",
      "Dokumen: *muhammad*\n",
      "Dokumen 1660:\n",
      "Skor Jaccard similarity: 1.0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\USER\\OneDrive\\Desktop\\SEMESTER 5\\Akademik\\PI-kelompok6\\hasil.ipynb Cell 1\u001b[0m line \u001b[0;36m<cell line: 36>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/OneDrive/Desktop/SEMESTER%205/Akademik/PI-kelompok6/hasil.ipynb#W0sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDokumen \u001b[39m\u001b[39m{\u001b[39;00mi \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/OneDrive/Desktop/SEMESTER%205/Akademik/PI-kelompok6/hasil.ipynb#W0sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mSkor Jaccard similarity: \u001b[39m\u001b[39m{\u001b[39;00mjaccard_scores[i]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/USER/OneDrive/Desktop/SEMESTER%205/Akademik/PI-kelompok6/hasil.ipynb#W0sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNama Dokumen: \u001b[39m\u001b[39m{\u001b[39;00mdocument_names[i]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/OneDrive/Desktop/SEMESTER%205/Akademik/PI-kelompok6/hasil.ipynb#W0sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m highlighted_document \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39msub(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/OneDrive/Desktop/SEMESTER%205/Akademik/PI-kelompok6/hasil.ipynb#W0sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mb(\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m|\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(tokenized_query) \u001b[39m+\u001b[39m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m1*\u001b[39m\u001b[39m'\u001b[39m, clean_documents[i])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USER/OneDrive/Desktop/SEMESTER%205/Akademik/PI-kelompok6/hasil.ipynb#W0sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDokumen: \u001b[39m\u001b[39m{\u001b[39;00mhighlighted_document\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import heapq\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import tokenized\n",
    "\n",
    "clean_documents = tokenized.tokenized_clean_document\n",
    "document_names = [os.path.basename(file) for file in tokenized.files]\n",
    "\n",
    "query = input(\"Masukkan kata yang ingin dicari: \")\n",
    "\n",
    "# Function to calculate Jaccard similarity\n",
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "# Tokenize the query\n",
    "tokenized_query = set(nltk.word_tokenize(query))\n",
    "\n",
    "# Tokenize the documents and calculate Jaccard similarity scores\n",
    "jaccard_scores = [jaccard_similarity(tokenized_query, set(nltk.word_tokenize(doc))) for doc in clean_documents]\n",
    "\n",
    "n = int(input(\"Masukkan jumlah dokumen yang ingin ditampilkan: \"))\n",
    "\n",
    "# Get the indices of the highest scoring documents based on Jaccard similarity\n",
    "jaccard_indices = heapq.nlargest(n, range(len(jaccard_scores)), jaccard_scores.__getitem__)\n",
    "\n",
    "# Print the ranked documents and the word positions based on Jaccard similarity\n",
    "print(f\"Dokumen teratas untuk query '{query}' berdasarkan Jaccard similarity adalah:\")\n",
    "for i in jaccard_indices:\n",
    "    print(f\"Dokumen {i + 1}:\")\n",
    "    print(f\"Skor Jaccard similarity: {jaccard_scores[i]}\")\n",
    "    print(f\"Nama Dokumen: {document_names[i]}\")\n",
    "\n",
    "    highlighted_document = re.sub(\n",
    "        r'\\b(' + '|'.join(tokenized_query) + r')\\b', r'*\\1*', clean_documents[i])\n",
    "    print(f\"Dokumen: {highlighted_document}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
